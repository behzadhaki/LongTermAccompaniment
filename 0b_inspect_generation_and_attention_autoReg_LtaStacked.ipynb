{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T09:47:03.396181Z",
     "start_time": "2024-08-01T09:47:03.393237Z"
    }
   },
   "cell_type": "code",
   "source": "# in this one",
   "id": "1341378031441c1e",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T09:47:03.397084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from model.LTA_Stacked import LTA_Stacked\n",
    "from model import load_model\n",
    "import torch\n",
    "from hvo_sequence.hvo_seq import HVO_Sequence\n",
    "from hvo_sequence.drum_mappings import ROLAND_REDUCED_MAPPING\n",
    "from bokeh.plotting import show, output_notebook\n",
    "import os\n",
    "import timeit\n",
    "from hvo_sequence.hvo_seq import HVO_Sequence\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "# mdl_ = 'misc/LTA_Stacked/Bass (LTA_Stacked) Predict 1 bar ahead, no velocity at inputs_ojodqmhh/060.pth'\n",
    "mdl_ = 'misc/LTA_Stacked/[bassGuiSynLeadPian] (LTA_Stacked) 1bar no in vel_m62skfv9/015.pth'\n",
    "\n",
    "model = load_model(\n",
    "    model_path=mdl_,\n",
    "    model_class=LTA_Stacked,\n",
    "    device='cpu',\n",
    "    is_evaluating=True\n",
    ")\n",
    "\n",
    "model.serialize(save_folder=os.path.dirname(mdl_), filename=mdl_.split('/')[-1].replace('.pth', '.pt'))\n",
    "model.eval()"
   ],
   "id": "f7efec53ebfdf8dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load data\n",
    "from data import StackedLTADatasetV2\n",
    "max_n_bars = 32\n",
    "\n",
    "# test_datasets:\n",
    "#   - \"data/lmd/data_bass_groove_test.bz2\"\n",
    "#   - \"data/lmd/data_guitar_groove_test.bz2\"\n",
    "#   - \"data/lmd/data_synth_groove_test.bz2\"\n",
    "#   - \"data/lmd/data_lead_groove_test.bz2\"\n",
    "#   - \"data/lmd/data_piano_groove_test.bz2\"\n",
    "\n",
    "dataset_dict = {\n",
    "    'bass': \"data/lmd/data_bass_groove_test.bz2\",\n",
    "    'guitar': \"data/lmd/data_guitar_groove_test.bz2\",\n",
    "    'synth': \"data/lmd/data_synth_groove_test.bz2\",\n",
    "    'lead': \"data/lmd/data_lead_groove_test.bz2\",\n",
    "    'piano': \"data/lmd/data_piano_groove_test.bz2\"\n",
    "}\n",
    "\n",
    "test_dataset = StackedLTADatasetV2(\n",
    "        input_inst_dataset_bz2_filepath=dataset_dict['guitar'],\n",
    "        output_inst_dataset_bz2_filepath=\"data/lmd/data_drums_full_unsplit.bz2\",\n",
    "        shift_tgt_by_n_steps=1,\n",
    "        max_input_bars=max_n_bars,\n",
    "        hop_n_bars=4,\n",
    "       input_has_velocity=False\n",
    "    )"
   ],
   "id": "6cbddb83677f0330",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = 'cpu'\n",
    "\n",
    "def patch_attention(m):\n",
    "    forward_orig = m.forward\n",
    "\n",
    "    def wrap(*args, **kwargs):\n",
    "        kwargs['need_weights'] = True\n",
    "        kwargs['average_attn_weights'] = False\n",
    "\n",
    "        return forward_orig(*args, **kwargs)\n",
    "\n",
    "    m.forward = wrap\n",
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "\n",
    "    def __call__(self, module, module_in, module_out):\n",
    "        self.outputs.append(module_out[1])\n",
    "\n",
    "    def clear(self):\n",
    "        self.outputs = []\n",
    "\n",
    "\n",
    "\n",
    "# plot heatmap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_hvo_seqs(groove_hvo__, drum_hvo__):\n",
    "    if len(groove_hvo__.shape) == 3:\n",
    "        groove_hvo_ = groove_hvo__[0]\n",
    "    else:\n",
    "        groove_hvo_ = groove_hvo__\n",
    "\n",
    "    if len(drum_hvo__.shape) == 3:\n",
    "        drum_hvo = drum_hvo__[0]\n",
    "    else:\n",
    "        drum_hvo = drum_hvo__\n",
    "\n",
    "    hvo_seq_bass = HVO_Sequence(\n",
    "        beat_division_factors=[4],\n",
    "        drum_mapping={'BASS Rhythm': [80]}\n",
    "    )\n",
    "    hvo_seq_bass.add_tempo(0, 120)\n",
    "    hvo_seq_bass.add_time_signature(0, 4, 4)\n",
    "    hvo_seq_bass.hvo = groove_hvo_.detach().numpy()\n",
    "\n",
    "    hvo_seq = HVO_Sequence(\n",
    "        beat_division_factors=[4],\n",
    "        drum_mapping=ROLAND_REDUCED_MAPPING\n",
    "    )\n",
    "    hvo_seq.add_tempo(0, 120)\n",
    "    hvo_seq.add_time_signature(0, 4, 4)\n",
    "    hvo_seq.hvo = drum_hvo.detach().numpy()\n",
    "\n",
    "    audio_bass = hvo_seq_bass.synthesize(sf_path='hvo_sequence/soundfonts/Standard_Drum_Kit.sf2') * 0.5\n",
    "    audio_drums = hvo_seq.synthesize(sf_path='hvo_sequence/soundfonts/Standard_Drum_Kit.sf2')\n",
    "    audio_mixed = audio_bass[:min(audio_bass.shape[0], audio_drums.shape[0])] + audio_drums[:min(audio_bass.shape[0], audio_drums.shape[0])]\n",
    "\n",
    "    return hvo_seq_bass, hvo_seq, audio_bass, audio_drums, audio_mixed\n",
    "\n",
    "\n",
    "def batch_data_extractor(data_, device=device):\n",
    "    stacked_target_shifted = data_[0].to(device)\n",
    "    stacked_target = data_[1].to(device)\n",
    "    return stacked_target_shifted, stacked_target\n",
    "\n",
    "def forward_using_batch_data(batch_data, scope_end_step=None, model_=model, device=device):\n",
    "    model_.train()\n",
    "\n",
    "    stacked_target_shifted, stacked_target = batch_data_extractor(\n",
    "        data_=batch_data,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    if scope_end_step is not None:\n",
    "        scope_end_step = min(scope_end_step, stacked_target_shifted.shape[1])\n",
    "        stacked_target_shifted = stacked_target_shifted[:, :scope_end_step, :]\n",
    "        stacked_target = stacked_target[:, :scope_end_step, :]\n",
    "\n",
    "    h_logits, v_logits, o_logits = model_.forward(shifted_tgt=stacked_target_shifted)\n",
    "\n",
    "    return h_logits, v_logits, o_logits, stacked_target.to(device)\n",
    "\n",
    "\n",
    "# Auto-regressive prediction\n",
    "def predict_using_batch_auto_reg(batch_data, model_=model, device='cpu'):\n",
    "    model_.eval()\n",
    "\n",
    "\n",
    "    stacked_target_shifted, stacked_target = batch_data_extractor(\n",
    "        data_=batch_data,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    generated = torch.zeros_like(stacked_target)\n",
    "    generated_shifted = torch.zeros_like(stacked_target_shifted)\n",
    "\n",
    "    generated_shifted[:, :16, ::10] = stacked_target_shifted[:, :16, ::10] # copy the first bar of bass only\n",
    "\n",
    "    for i in range(16, stacked_target_shifted.shape[1]):\n",
    "        h_logits, v_logits, o_logits = model_.forward(shifted_tgt=generated_shifted)\n",
    "\n",
    "        h = torch.sigmoid(h_logits)\n",
    "        h[:, :, 0][h[:, :, 0] < 0.2] = 0.0 # bass must be confident\n",
    "        h[:, :, 1:][h[:, :, 1:] < 0.2] = 0.0    # more improve allowed for drums\n",
    "        # h[h >= 0.4] = 1.0\n",
    "        # sample\n",
    "        h = torch.bernoulli(h)\n",
    "        v = torch.clamp(((torch.tanh(v_logits) + 1.0) / 2), 0.0, 1.0) * h\n",
    "        o = torch.tanh(o_logits) * h\n",
    "\n",
    "        generated[:, i, :10] = h[:, i, :]\n",
    "        generated[:, i, 10:20] = v[:, i, :]\n",
    "        generated[:, i, 20:] = o[:, i, :]\n",
    "\n",
    "        if i < stacked_target_shifted.shape[1] - 1:\n",
    "            generated_shifted[:, i+1, 0] = stacked_target_shifted[:, i+1, 0] # use the gt bass\n",
    "            generated_shifted[:, i+1, 10] = stacked_target_shifted[:, i+1, 10] # use the gt bass\n",
    "            generated_shifted[:, i+1, 1:10] = h[:, i, 1:]\n",
    "            generated_shifted[:, i+1, 11:20] = o[:, i, 1:]\n",
    "\n",
    "\n",
    "    drum_hvo = torch.zeros((generated.shape[0], generated.shape[1], 27))\n",
    "\n",
    "    groove_hvo = generated[:, :, ::10]\n",
    "    grove_hvo_tgt = stacked_target[:, :, ::10]\n",
    "\n",
    "    drum_hvo[:, :, :9] = generated[:, :, 1:10]\n",
    "    drum_hvo[:, :, 9:18] = generated[:, :, 11:20]\n",
    "    drum_hvo[:, :, 18:] = generated[:, :, 21:]\n",
    "\n",
    "    drum_hvo_tgt = torch.zeros_like(drum_hvo)\n",
    "    drum_hvo_tgt[:, :, :9] = stacked_target[:, :, 1:10]\n",
    "    drum_hvo_tgt[:, :, 9:18] = stacked_target[:, :, 11:20]\n",
    "    drum_hvo_tgt[:, :, 18:] = stacked_target[:, :, 21:]\n",
    "\n",
    "    return groove_hvo, drum_hvo, grove_hvo_tgt, drum_hvo_tgt\n",
    "\n",
    "\n",
    "sample_ix = torch.randint(0, len(test_dataset), (1,)).item()\n",
    "groove_hvo, drum_hvo, grove_hvo_tgt, drum_hvo_tgt = predict_using_batch_auto_reg(test_dataset[sample_ix:sample_ix+1], model_=model, device='cpu')\n",
    "hvo_seq_bass, hvo_seq_drum, audio_bass, audio_drums, audio_mixed = get_hvo_seqs(groove_hvo, drum_hvo)\n",
    "hvo_seq_bass_tgt, hvo_seq_drum_tgt, _, _, audio_mixed_tgt = get_hvo_seqs(grove_hvo_tgt, drum_hvo_tgt)"
   ],
   "id": "ead7e3480bd12ecd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from IPython.display import Audio, display\n",
    "print(\"---------------------------------\")\n",
    "print(\"Generated\")\n",
    "print(\"---------------------------------\")\n",
    "display(Audio(audio_mixed, rate=44100))\n",
    "hvo_seq_bass.to_html_plot(show_figure=True, width=1400, height=200)"
   ],
   "id": "75a769faf19c91d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "hvo_seq_drum.to_html_plot(show_figure=True, width=1400, height=200)",
   "id": "15e7591ab93e0e51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"---------------------------------\")\n",
    "print(\"Target\")\n",
    "print(\"---------------------------------\")\n",
    "display(Audio(audio_mixed_tgt, rate=44100))\n",
    "hvo_seq_bass_tgt.to_html_plot(show_figure=True, width=1400, height=200)"
   ],
   "id": "d4807436302c276e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "hvo_seq_drum_tgt.to_html_plot(show_figure=True, width=1400, height=200)",
   "id": "57036a5e8da8efa3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_head_n_attention(head_n, attn_weights_list,  binary_vis=False):\n",
    "    attn_weights = attn_weights_list[0] if isinstance(attn_weights_list, list) else attn_weights_list\n",
    "    \n",
    "    print(attn_weights.shape)\n",
    "    if head_n is None:\n",
    "        # sum across all heads\n",
    "\n",
    "        attn_weights_0 = attn_weights.sum(dim=1, keepdim=True).detach().cpu().numpy()\n",
    "    else:\n",
    "\n",
    "        attn_weights_0 = attn_weights.detach().cpu().numpy()\n",
    "    \n",
    "        \n",
    "    if binary_vis:\n",
    "        attn_weights_0 = attn_weights_0 > 0.01\n",
    "        \n",
    "\n",
    "    # two side by side plots\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
    "\n",
    "    # use a greyscale color map\n",
    "    sns.heatmap(attn_weights_0[0, head_n if head_n is not None else 0, :, :].transpose(), cmap='Greys', ax=ax[0])\n",
    "    ax[0].set_title('Cross-Attention Weights - With instrumental')\n",
    "\n",
    "    # put ticks on 0, 4, ...\n",
    "    # rotate y ticks\n",
    "\n",
    "    for a in ax:\n",
    "        a.set_xticks([i for i in range(0, max_n_bars*16, 16)])\n",
    "        a.set_xticklabels([i for i in range(0, max_n_bars*16, 16)])\n",
    "        a.set_yticks([i for i in range(0, max_n_bars*16, 16)])\n",
    "        a.set_yticklabels([i for i in range(0, max_n_bars*16, 16)])\n",
    "        a.set_yticklabels(a.get_yticklabels(), rotation=0)\n",
    "\n",
    "    # flip y axis\n",
    "    for a in ax:\n",
    "        a.invert_yaxis()\n",
    "    \n",
    "    # ensure same aspect ratio\n",
    "    for a in ax:\n",
    "        a.set_aspect('equal')\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "save_output = SaveOutput()\n",
    "\n",
    "layer_ix = 0\n",
    "\n",
    "patch_attention(model.TransformerEncoder.layers[layer_ix].self_attn)\n",
    "hook_handle = model.TransformerEncoder.layers[layer_ix].self_attn.register_forward_hook(save_output)\n",
    "\n",
    "groove_hvo, drum_hvo, grove_hvo_tgt, drum_hvo_tgt = predict_using_batch_auto_reg(test_dataset[sample_ix:sample_ix+1], model_=model, device='cpu')\n",
    "\n",
    "\n",
    "plot_head_n_attention(0, save_output.outputs[-1], binary_vis=True)\n"
   ],
   "id": "a0888dc4f783c7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(save_output.outputs)",
   "id": "909afae9f7c9f18d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "save_output.outputs[-1].shape",
   "id": "a69733740d0ce021",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # alternative positional embedding\n",
    "#\n",
    "# import numpy as np\n",
    "#\n",
    "# def modified_positional_encoding(period, d_model, max_len=512):\n",
    "#     pe = np.zeros((max_len, d_model))\n",
    "#     position = np.arange(0, max_len).reshape(-1, 1)\n",
    "#\n",
    "#     # Sinusoidal part\n",
    "#     div_term = 2 * np.pi / period\n",
    "#     pe[:, 0::2] = np.sin(position * div_term)\n",
    "#     pe[:, 1::2] = np.cos(position * div_term)\n",
    "#\n",
    "#     # Linear term for distinguishability\n",
    "#     div_term_linear = 10000 ** (np.arange(0, d_model) / d_model)\n",
    "#     pe += position / div_term_linear\n",
    "#\n",
    "#     return pe\n",
    "#\n",
    "# def normal_positional_encoding(d_model, max_len=512):\n",
    "#     pe = np.zeros((max_len, d_model))\n",
    "#     position = np.arange(0, max_len).reshape(-1, 1)\n",
    "#\n",
    "#     # Sinusoidal part\n",
    "#     div_term = 10000 ** (np.arange(0, d_model, 2) / d_model)\n",
    "#     pe[:, 0::2] = np.sin(position / div_term)\n",
    "#     pe[:, 1::2] = np.cos(position / div_term)\n",
    "#\n",
    "#     return pe\n",
    "#\n",
    "# # Example usage:\n",
    "# period = 16  # For example, if the period is 24\n",
    "# d_model = 16\n",
    "# pos_enc_periodic = modified_positional_encoding(period, d_model)\n",
    "# pos_enc_normal = normal_positional_encoding(d_model)\n",
    "#\n",
    "# # To use this positional encoding in a transformer model\n",
    "# import torch\n",
    "# pos_enc_tensor_periodic = torch.tensor(pos_enc_periodic, dtype=torch.float32)\n",
    "# pos_enc_tensor_normal = torch.tensor(pos_enc_normal, dtype=torch.float32)\n",
    "#\n",
    "#\n",
    "#\n",
    "# # visualize similar to attention is all you need paper\n",
    "# # show both\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "#\n",
    "# fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
    "#\n",
    "# # use a greyscale color map\n",
    "# sns.heatmap(pos_enc_tensor_periodic, cmap='Greys', ax=ax[0])\n",
    "# ax[0].set_title('Modified Positional Encoding')\n",
    "#\n",
    "# sns.heatmap(pos_enc_tensor_normal, cmap='Greys', ax=ax[1])\n",
    "# ax[1].set_title('Normal Positional Encoding')\n",
    "#\n",
    "# plt.show()\n"
   ],
   "id": "2852cac194d4b6e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b07a0add472cd548",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GrooveTransformer",
   "language": "python",
   "name": "groovetransformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
